4/15/2025
We finished up data cleaning and transformation today for our uses later in the project
This included fixing column names and creation of vector representations of the emotions/activites
present in each journal entry. We then did basic modeling using logistic regression and TD-if 
vectorization. 
The resulting accuracy is below
F1 Score (Macro): 0.12191325514387374
Precision (Micro): 0.7745901639344263
Recall (Micro): 0.21380090497737556
Exact Match Accuracy: 0.030508474576271188

4/24/2025
We worked on the fine-tuned DistilBERT model. Used Binary Cross-Entropy with Logits as the loss function.
Training Setup: 4 epochs, batch size 8, and learning rate 2e-5.
The resulting accuracy is below during epoch 4:
Training Loss: 0.1899
Eval Loss: 0.1832
F1 Micro: 0.5323
F1 Macro: 0.2736

Below is the classification report:
precision    recall  f1-score   support

      afraid       0.00      0.00      0.00         2
       angry       0.00      0.00      0.00         5
     anxious       0.50      0.04      0.08        23
     ashamed       0.00      0.00      0.00         4
     awkward       0.00      0.00      0.00         4
       bored       0.00      0.00      0.00        15
        calm       0.75      0.10      0.18        87
    confused       0.00      0.00      0.00         7
   disgusted       0.00      0.00      0.00         3
     excited       0.00      0.00      0.00        46
  frustrated       0.60      0.21      0.32        28
       happy       0.73      0.73      0.73       153
     jealous       0.00      0.00      0.00         0
   nostalgic       0.00      0.00      0.00         6
       proud       0.82      0.14      0.23        66
         sad       0.00      0.00      0.00         6
   satisfied       0.49      0.28      0.36       124
   surprised       0.00      0.00      0.00        10
    exercise       0.86      0.74      0.79        34
      family       0.90      0.68      0.77        53
        food       0.95      0.88      0.91        41
     friends       1.00      0.45      0.62        20
         god       1.00      0.42      0.59        19
      health       1.00      0.27      0.42        26
        love       0.00      0.00      0.00        10
  recreation       1.00      0.06      0.12        16
      school       0.00      0.00      0.00         1
       sleep       0.96      0.93      0.95        28
        work       0.90      0.81      0.85        47

   micro avg       0.78      0.40      0.53       884
   macro avg       0.43      0.23      0.27       884
weighted avg       0.66      0.40      0.46       884
 samples avg       0.69      0.42      0.50       884

This had strong performance for common classes like 'happy' but poor coverage for low frequency
emotions. To enhance the model, we added class weights, lowered the learning 
rate, had a learning rate scheduler, and increased epochs to 10 with early stopping. The best epoch
was 10 with the following result accuracy:
Training Loss: 0.3744
Eval Loss: 0.5491
F1 Micro: 0.5856
F1 Macro: 0.4784
F1 Weighted: 0.6143

Below is the classification report:
              precision    recall  f1-score   support

      afraid       0.09      1.00      0.16         2
       angry       0.12      0.60      0.20         5
     anxious       0.42      0.74      0.54        23
     ashamed       0.14      0.75      0.23         4
     awkward       0.07      0.25      0.11         4
       bored       0.32      0.53      0.40        15
        calm       0.54      0.49      0.52        87
    confused       0.19      0.57      0.29         7
   disgusted       0.04      0.33      0.08         3
     excited       0.33      0.17      0.23        46
  frustrated       0.46      0.68      0.55        28
       happy       0.74      0.69      0.71       153
     jealous       0.00      0.00      0.00         0
   nostalgic       0.16      0.50      0.24         6
       proud       0.47      0.47      0.47        66
         sad       0.14      0.67      0.24         6
   satisfied       0.48      0.59      0.53       124
   surprised       0.00      0.00      0.00        10
    exercise       0.73      0.97      0.84        34
      family       0.82      0.89      0.85        53
        food       0.95      0.90      0.93        41
     friends       0.76      0.65      0.70        20
         god       0.94      0.79      0.86        19
      health       0.67      0.62      0.64        26
        love       0.43      0.60      0.50        10
  recreation       0.71      0.62      0.67        16
      school       0.50      1.00      0.67         1
       sleep       0.79      0.96      0.87        28
        work       0.84      0.89      0.87        47

   micro avg       0.53      0.65      0.59       884
   macro avg       0.44      0.62      0.48       884
weighted avg       0.60      0.65      0.61       884
 samples avg       0.61      0.67      0.60       884

Per-Class Accuracy:
afraid: 0.9288
angry: 0.9186
anxious: 0.9017
ashamed: 0.9322
awkward: 0.9458
bored: 0.9186
calm: 0.7288
confused: 0.9322
disgusted: 0.9186
excited: 0.8169
frustrated: 0.8949
happy: 0.7119
jealous: 0.9831
nostalgic: 0.9356
proud: 0.7627
sad: 0.9119
satisfied: 0.5593
surprised: 0.9322
exercise: 0.9559
family: 0.9458
food: 0.9797
friends: 0.9627
god: 0.9831
health: 0.9390
love: 0.9593
recreation: 0.9661
school: 0.9966
sleep: 0.9729
work: 0.9559

The underrepresented classes saw boosts in recall and precision. 

//
Things we can try to improve the model:
- Use validation set to try diff thresholds per label instead of 0.5.
- Split data into train, val, test sets and use validation set to monitor F1, tune thresholds,
and select best model. 
- Since rare labels are underrepresented, use 'WeightedRandomSampler'??
- could try bert-base-uncased, roberta-base, or GoEmotions instead of DistilBERT
//

We analyzed how different topics co-occur with various emotions in journal entries.
We wrote code to build a co-occurence matrix that counts how often each emotions appears
alongside each topic. Then, it normalizes these coutns to show the distribution of topics 
within each emotion. We visualized using heatmaps.

//
We can do bar plots per emotion. We can also do logistic regression or Chi-square tests??
//
4/29/2024
These are results from the training using weighted random sampling
It doesn't seem to have helped at all. Loss even went up on the last epoch which wasn't seen in the previous model and the accuracy/
F1 scores both were lower. 
It seems to have helped wtih one or two labels that had very poor accuracy on the previous model like 
the emotion nostalgic but most others were pretty similar. 

Epoch 10/10
Training: 100%|██████████| 148/148 [31:03<00:00, 12.59s/it]
Training Loss: 0.4087
Evaluating: 100%|██████████| 37/37 [02:03<00:00,  3.33s/it]
Eval Loss: 0.5742
F1 Micro: 0.5342
F1 Macro: 0.4747
F1 Weighted: 0.5668

Best model was from epoch 9 with F1 micro of 0.5479

 Classification Report:

              precision    recall  f1-score   support

      afraid       0.14      0.50      0.22         2
       angry       0.18      0.40      0.25         5
     anxious       0.38      0.78      0.51        23
     ashamed       0.40      0.50      0.44         4
     awkward       0.09      0.25      0.13         4
       bored       0.30      0.53      0.38        15
        calm       0.53      0.36      0.43        87
    confused       0.25      0.29      0.27         7
   disgusted       0.00      0.00      0.00         3
     excited       0.22      0.48      0.31        46
  frustrated       0.50      0.64      0.56        28
       happy       0.74      0.69      0.71       153
     jealous       0.00      0.00      0.00         0
   nostalgic       0.05      0.83      0.10         6
       proud       0.49      0.53      0.51        66
         sad       0.17      0.67      0.27         6
   satisfied       0.49      0.68      0.57       124
   surprised       0.06      0.20      0.10        10
    exercise       0.82      0.94      0.88        34
      family       0.81      0.57      0.67        53
        food       0.94      0.83      0.88        41
     friends       0.64      0.80      0.71        20
         god       0.84      0.84      0.84        19
      health       0.59      0.62      0.60        26
        love       0.16      0.70      0.25        10
  recreation       0.50      0.62      0.56        16
      school       0.50      1.00      0.67         1
       sleep       0.81      0.93      0.87        28
        work       0.83      0.74      0.79        47

   micro avg       0.48      0.64      0.55       884
   macro avg       0.43      0.58      0.46       884
weighted avg       0.59      0.64      0.60       884
 samples avg       0.52      0.65      0.54       884


 Per-Class Accuracy:
afraid: 0.9763
angry: 0.9593
anxious: 0.8814
ashamed: 0.9831
awkward: 0.9559
bored: 0.9119
calm: 0.7186
confused: 0.9627
disgusted: 0.9729
excited: 0.6610
frustrated: 0.9051
happy: 0.7119
jealous: 0.9932
nostalgic: 0.7017
proud: 0.7729
sad: 0.9254
satisfied: 0.5695
surprised: 0.8746
exercise: 0.9695
family: 0.8983
food: 0.9695
friends: 0.9559
god: 0.9797
health: 0.9288
love: 0.8610
recreation: 0.9458
school: 0.9966
sleep: 0.9729
work: 0.9356

