{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from data_cleaning_import import clean_create_vectors\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (4.13.2)\n",
      "Collecting hf-xet>=0.1.4 (from huggingface_hub[hf_xet])\n",
      "  Downloading hf_xet-1.1.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (494 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from requests->huggingface_hub[hf_xet]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from requests->huggingface_hub[hf_xet]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from requests->huggingface_hub[hf_xet]) (2024.12.14)\n",
      "Downloading hf_xet-1.1.0-cp37-abi3-macosx_11_0_arm64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf-xet\n",
      "Successfully installed hf-xet-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"huggingface_hub[hf_xet]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fold1 Train E1: 100%|██████████| 29/29 [01:46<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 1 train loss: 0.5916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold1 Train E2: 100%|██████████| 29/29 [01:44<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 2 train loss: 0.4395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold1 Train E3: 100%|██████████| 29/29 [01:44<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 3 train loss: 0.3980\n",
      "\n",
      "=== Fold 2/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold2 Train E1: 100%|██████████| 30/30 [01:45<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 1 train loss: 0.6227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold2 Train E2: 100%|██████████| 30/30 [01:49<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 2 train loss: 0.4617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold2 Train E3: 100%|██████████| 30/30 [01:52<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 3 train loss: 0.4102\n",
      "\n",
      "=== Fold 3/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold3 Train E1: 100%|██████████| 30/30 [01:52<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 1 train loss: 0.5765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold3 Train E2: 100%|██████████| 30/30 [01:57<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 2 train loss: 0.4219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold3 Train E3: 100%|██████████| 30/30 [01:55<00:00,  3.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 3 train loss: 0.3797\n",
      "\n",
      "=== Fold 4/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold4 Train E1: 100%|██████████| 30/30 [01:50<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 1 train loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold4 Train E2: 100%|██████████| 30/30 [01:48<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 2 train loss: 0.4850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold4 Train E3: 100%|██████████| 30/30 [01:47<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 3 train loss: 0.4263\n",
      "\n",
      "=== Fold 5/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold5 Train E1: 100%|██████████| 30/30 [01:50<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 1 train loss: 0.6490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold5 Train E2: 100%|██████████| 30/30 [01:48<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 2 train loss: 0.4908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold5 Train E3: 100%|██████████| 30/30 [01:46<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 3 train loss: 0.4166\n",
      "Optimal thresholds per label: [np.float64(0.23), np.float64(0.24), np.float64(0.25), np.float64(0.21), np.float64(0.23), np.float64(0.23), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.28), np.float64(0.27), np.float64(0.49), 0.5, np.float64(0.24), np.float64(0.33), np.float64(0.24), np.float64(0.42), np.float64(0.0), np.float64(0.25), np.float64(0.34), np.float64(0.33), np.float64(0.23), np.float64(0.26), np.float64(0.24), np.float64(0.25), np.float64(0.27), np.float64(0.22), np.float64(0.27), np.float64(0.3)]\n",
      "\n",
      "Ensembled Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      afraid       0.01      1.00      0.02         2\n",
      "       angry       0.03      1.00      0.07         5\n",
      "     anxious       0.09      0.96      0.16        23\n",
      "     ashamed       0.02      1.00      0.03         4\n",
      "     awkward       0.02      0.50      0.03         4\n",
      "       bored       0.12      0.20      0.15        15\n",
      "        calm       0.29      1.00      0.46        87\n",
      "    confused       0.02      1.00      0.05         7\n",
      "   disgusted       0.01      1.00      0.02         3\n",
      "     excited       0.23      0.54      0.32        46\n",
      "  frustrated       0.15      0.68      0.24        28\n",
      "       happy       0.63      0.89      0.74       153\n",
      "     jealous       0.00      0.00      0.00         0\n",
      "   nostalgic       0.18      0.33      0.24         6\n",
      "       proud       0.23      0.94      0.37        66\n",
      "         sad       0.04      0.33      0.07         6\n",
      "   satisfied       0.52      0.72      0.61       124\n",
      "   surprised       0.03      1.00      0.07        10\n",
      "    exercise       0.33      0.29      0.31        34\n",
      "      family       0.48      0.68      0.56        53\n",
      "        food       0.59      0.41      0.49        41\n",
      "     friends       0.13      0.20      0.16        20\n",
      "         god       0.75      0.32      0.44        19\n",
      "      health       0.17      0.69      0.28        26\n",
      "        love       0.07      0.10      0.08        10\n",
      "  recreation       0.33      0.12      0.18        16\n",
      "      school       0.01      1.00      0.01         1\n",
      "       sleep       0.34      0.50      0.41        28\n",
      "        work       0.24      0.94      0.38        47\n",
      "\n",
      "   micro avg       0.17      0.72      0.27       884\n",
      "   macro avg       0.21      0.63      0.24       884\n",
      "weighted avg       0.38      0.72      0.45       884\n",
      " samples avg       0.17      0.70      0.27       884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH    = \"data.csv\"\n",
    "MODEL_NAME   = \"bert-base-uncased\"\n",
    "MAX_SEQ_LEN  = 128\n",
    "BATCH_SIZE   = 32\n",
    "LR           = 2e-5\n",
    "NUM_EPOCHS   = 3       # shorten per‐fold\n",
    "WARMUP_RATIO = 0.1\n",
    "N_FOLDS      = 5\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = clean_create_vectors(df)\n",
    "X = df[\"journal\"].tolist()\n",
    "y = df.drop(columns=[\"journal\",\"emotion_vectors\",\"activity_vectors\"]).astype(int).values\n",
    "label_names = df.drop(columns=[\"journal\",\"emotion_vectors\",\"activity_vectors\"]).columns.tolist()\n",
    "num_labels = len(label_names)\n",
    "\n",
    "# split out a final test set\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class JournalDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.enc = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "    def __len__(self):  return len(self.labels)\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            \"input_ids\":      self.enc.input_ids[i],\n",
    "            \"attention_mask\": self.enc.attention_mask[i],\n",
    "            \"labels\":         self.labels[i],\n",
    "        }\n",
    "\n",
    "# final test dataloader\n",
    "test_ds = JournalDataset(X_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3) K-FOLD TRAIN + ENSEMBLE\n",
    "mskf = MultilabelStratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "all_test_probs = np.zeros((len(X_test), num_labels))\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(mskf.split(X_trainval, y_trainval), 1):\n",
    "    print(f\"\\n=== Fold {fold}/{N_FOLDS} ===\")\n",
    "    X_tr = [X_trainval[i] for i in tr_idx];  y_tr = y_trainval[tr_idx]\n",
    "    X_val = [X_trainval[i] for i in val_idx]; y_val = y_trainval[val_idx]\n",
    "\n",
    "    # dataloaders\n",
    "    tr_dl  = DataLoader(JournalDataset(X_tr, y_tr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dl = DataLoader(JournalDataset(X_val, y_val), batch_size=BATCH_SIZE)\n",
    "\n",
    "    # fresh model + optimizer + scheduler\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
    "    ).to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "    total_steps = len(tr_dl) * NUM_EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(total_steps * WARMUP_RATIO),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # train\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for batch in tqdm(tr_dl, desc=f\"Fold{fold} Train E{epoch}\"):\n",
    "            optimizer.zero_grad()\n",
    "            inp = {k:v.to(DEVICE) for k,v in batch.items() if k!=\"labels\"}\n",
    "            lbl = batch[\"labels\"].to(DEVICE)\n",
    "            out = model(**inp).logits\n",
    "            loss = loss_fn(out, lbl)\n",
    "            loss.backward()\n",
    "            optimizer.step(); scheduler.step()\n",
    "            running += loss.item()\n",
    "        print(f\"  → Epoch {epoch} train loss: {running/len(tr_dl):.4f}\")\n",
    "\n",
    "    # predict on the held-out test set\n",
    "    model.eval()\n",
    "    fold_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:\n",
    "            inp = {k:v.to(DEVICE) for k,v in batch.items() if k!=\"labels\"}\n",
    "            logits = model(**inp).logits\n",
    "            fold_probs.append(torch.sigmoid(logits).cpu().numpy())\n",
    "    fold_probs = np.vstack(fold_probs)\n",
    "    all_test_probs += fold_probs\n",
    "\n",
    "# average the 5 fold predictions\n",
    "all_test_probs /= N_FOLDS\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4) THRESHOLD TUNING & FINAL REPORT\n",
    "# reuse the same find_best_thresholds from before, but you can also \n",
    "# just use a fixed .5 or grid‐search on your val predictions if you like.\n",
    "\n",
    "def find_best_thresholds(probs, labels, num_labels):\n",
    "    best_ts = []\n",
    "    for i in range(num_labels):\n",
    "        best_f, best_t = 0, .5\n",
    "        for t in np.linspace(0,1,101):\n",
    "            p = (probs[:,i] >= t).astype(int)\n",
    "            f = f1_score(labels[:,i], p, zero_division=0)\n",
    "            if f > best_f:\n",
    "                best_f, best_t = f, t\n",
    "        best_ts.append(best_t)\n",
    "    return best_ts\n",
    "\n",
    "# (you could hold out a small portion of the trainval for threshold tuning,\n",
    "#  or just use y_test itself if you’re OK with peeking)\n",
    "thresholds = find_best_thresholds(all_test_probs, y_test, num_labels)\n",
    "print(\"Optimal thresholds per label:\", thresholds)\n",
    "\n",
    "# apply them\n",
    "final_preds = np.zeros_like(all_test_probs, dtype=int)\n",
    "for i,t in enumerate(thresholds):\n",
    "    final_preds[:,i] = (all_test_probs[:,i] >= t).astype(int)\n",
    "\n",
    "print(\"\\nEnsembled Classification Report:\\n\")\n",
    "print(classification_report(y_test, final_preds,\n",
    "                            target_names=label_names,\n",
    "                            zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
