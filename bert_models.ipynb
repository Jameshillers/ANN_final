{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from data_cleaning_import import clean_create_vectors\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fold1 Train E1: 100%|██████████| 29/29 [01:46<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 1 train loss: 0.5916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold1 Train E2: 100%|██████████| 29/29 [01:44<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 2 train loss: 0.4395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold1 Train E3: 100%|██████████| 29/29 [01:44<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 3 train loss: 0.3980\n",
      "\n",
      "=== Fold 2/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold2 Train E1: 100%|██████████| 30/30 [01:45<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 1 train loss: 0.6227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold2 Train E2: 100%|██████████| 30/30 [01:49<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 2 train loss: 0.4617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold2 Train E3: 100%|██████████| 30/30 [01:52<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 3 train loss: 0.4102\n",
      "\n",
      "=== Fold 3/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold3 Train E1: 100%|██████████| 30/30 [01:52<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 1 train loss: 0.5765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold3 Train E2: 100%|██████████| 30/30 [01:57<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 2 train loss: 0.4219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold3 Train E3: 100%|██████████| 30/30 [01:55<00:00,  3.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 3 train loss: 0.3797\n",
      "\n",
      "=== Fold 4/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold4 Train E1: 100%|██████████| 30/30 [01:50<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 1 train loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold4 Train E2: 100%|██████████| 30/30 [01:48<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 2 train loss: 0.4850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold4 Train E3: 100%|██████████| 30/30 [01:47<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 3 train loss: 0.4263\n",
      "\n",
      "=== Fold 5/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold5 Train E1: 100%|██████████| 30/30 [01:50<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 1 train loss: 0.6490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold5 Train E2: 100%|██████████| 30/30 [01:48<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 2 train loss: 0.4908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold5 Train E3: 100%|██████████| 30/30 [01:46<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Epoch 3 train loss: 0.4166\n",
      "Optimal thresholds per label: [np.float64(0.23), np.float64(0.24), np.float64(0.25), np.float64(0.21), np.float64(0.23), np.float64(0.23), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.28), np.float64(0.27), np.float64(0.49), 0.5, np.float64(0.24), np.float64(0.33), np.float64(0.24), np.float64(0.42), np.float64(0.0), np.float64(0.25), np.float64(0.34), np.float64(0.33), np.float64(0.23), np.float64(0.26), np.float64(0.24), np.float64(0.25), np.float64(0.27), np.float64(0.22), np.float64(0.27), np.float64(0.3)]\n",
      "\n",
      "Ensembled Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      afraid       0.01      1.00      0.02         2\n",
      "       angry       0.03      1.00      0.07         5\n",
      "     anxious       0.09      0.96      0.16        23\n",
      "     ashamed       0.02      1.00      0.03         4\n",
      "     awkward       0.02      0.50      0.03         4\n",
      "       bored       0.12      0.20      0.15        15\n",
      "        calm       0.29      1.00      0.46        87\n",
      "    confused       0.02      1.00      0.05         7\n",
      "   disgusted       0.01      1.00      0.02         3\n",
      "     excited       0.23      0.54      0.32        46\n",
      "  frustrated       0.15      0.68      0.24        28\n",
      "       happy       0.63      0.89      0.74       153\n",
      "     jealous       0.00      0.00      0.00         0\n",
      "   nostalgic       0.18      0.33      0.24         6\n",
      "       proud       0.23      0.94      0.37        66\n",
      "         sad       0.04      0.33      0.07         6\n",
      "   satisfied       0.52      0.72      0.61       124\n",
      "   surprised       0.03      1.00      0.07        10\n",
      "    exercise       0.33      0.29      0.31        34\n",
      "      family       0.48      0.68      0.56        53\n",
      "        food       0.59      0.41      0.49        41\n",
      "     friends       0.13      0.20      0.16        20\n",
      "         god       0.75      0.32      0.44        19\n",
      "      health       0.17      0.69      0.28        26\n",
      "        love       0.07      0.10      0.08        10\n",
      "  recreation       0.33      0.12      0.18        16\n",
      "      school       0.01      1.00      0.01         1\n",
      "       sleep       0.34      0.50      0.41        28\n",
      "        work       0.24      0.94      0.38        47\n",
      "\n",
      "   micro avg       0.17      0.72      0.27       884\n",
      "   macro avg       0.21      0.63      0.24       884\n",
      "weighted avg       0.38      0.72      0.45       884\n",
      " samples avg       0.17      0.70      0.27       884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH    = \"data.csv\"\n",
    "MODEL_NAME   = \"bert-base-uncased\"\n",
    "MAX_SEQ_LEN  = 128\n",
    "BATCH_SIZE   = 32\n",
    "LR           = 2e-5\n",
    "NUM_EPOCHS   = 3       \n",
    "WARMUP_RATIO = 0.1\n",
    "N_FOLDS      = 5\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = clean_create_vectors(df)\n",
    "X = df[\"journal\"].tolist()\n",
    "y = df.drop(columns=[\"journal\",\"emotion_vectors\",\"activity_vectors\"]).astype(int).values\n",
    "label_names = df.drop(columns=[\"journal\",\"emotion_vectors\",\"activity_vectors\"]).columns.tolist()\n",
    "num_labels = len(label_names)\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class JournalDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.enc = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "    def __len__(self):  return len(self.labels)\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            \"input_ids\":      self.enc.input_ids[i],\n",
    "            \"attention_mask\": self.enc.attention_mask[i],\n",
    "            \"labels\":         self.labels[i],\n",
    "        }\n",
    "\n",
    "test_ds = JournalDataset(X_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# k fold train\n",
    "mskf = MultilabelStratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "all_test_probs = np.zeros((len(X_test), num_labels))\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(mskf.split(X_trainval, y_trainval), 1):\n",
    "    print(f\"\\n=== Fold {fold}/{N_FOLDS} ===\")\n",
    "    X_tr = [X_trainval[i] for i in tr_idx];  y_tr = y_trainval[tr_idx]\n",
    "    X_val = [X_trainval[i] for i in val_idx]; y_val = y_trainval[val_idx]\n",
    "\n",
    "    # dataloaders\n",
    "    tr_dl  = DataLoader(JournalDataset(X_tr, y_tr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dl = DataLoader(JournalDataset(X_val, y_val), batch_size=BATCH_SIZE)\n",
    "\n",
    "    # fresh model + optimizer + scheduler\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
    "    ).to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "    total_steps = len(tr_dl) * NUM_EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(total_steps * WARMUP_RATIO),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # train\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for batch in tqdm(tr_dl, desc=f\"Fold{fold} Train E{epoch}\"):\n",
    "            optimizer.zero_grad()\n",
    "            inp = {k:v.to(DEVICE) for k,v in batch.items() if k!=\"labels\"}\n",
    "            lbl = batch[\"labels\"].to(DEVICE)\n",
    "            out = model(**inp).logits\n",
    "            loss = loss_fn(out, lbl)\n",
    "            loss.backward()\n",
    "            optimizer.step(); scheduler.step()\n",
    "            running += loss.item()\n",
    "        print(f\"  → Epoch {epoch} train loss: {running/len(tr_dl):.4f}\")\n",
    "\n",
    "    # predict on the held-out test set\n",
    "    model.eval()\n",
    "    fold_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:\n",
    "            inp = {k:v.to(DEVICE) for k,v in batch.items() if k!=\"labels\"}\n",
    "            logits = model(**inp).logits\n",
    "            fold_probs.append(torch.sigmoid(logits).cpu().numpy())\n",
    "    fold_probs = np.vstack(fold_probs)\n",
    "    all_test_probs += fold_probs\n",
    "\n",
    "# average the 5 fold predictions\n",
    "all_test_probs /= N_FOLDS\n",
    "\n",
    "\n",
    "#threshold tuning\n",
    "def find_best_thresholds(probs, labels, num_labels):\n",
    "    best_ts = []\n",
    "    for i in range(num_labels):\n",
    "        best_f, best_t = 0, .5\n",
    "        for t in np.linspace(0,1,101):\n",
    "            p = (probs[:,i] >= t).astype(int)\n",
    "            f = f1_score(labels[:,i], p, zero_division=0)\n",
    "            if f > best_f:\n",
    "                best_f, best_t = f, t\n",
    "        best_ts.append(best_t)\n",
    "    return best_ts\n",
    "\n",
    "# (you could hold out a small portion of the trainval for threshold tuning,\n",
    "#  or just use y_test itself if you’re OK with peeking)\n",
    "thresholds = find_best_thresholds(all_test_probs, y_test, num_labels)\n",
    "print(\"Optimal thresholds per label:\", thresholds)\n",
    "\n",
    "# apply them\n",
    "final_preds = np.zeros_like(all_test_probs, dtype=int)\n",
    "for i,t in enumerate(thresholds):\n",
    "    final_preds[:,i] = (all_test_probs[:,i] >= t).astype(int)\n",
    "\n",
    "print(\"\\nEnsembled Classification Report:\\n\")\n",
    "print(classification_report(y_test, final_preds,\n",
    "                            target_names=label_names,\n",
    "                            zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW \n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = \"data.csv\"\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "LR = 2e-5\n",
    "NUM_EPOCHS = 2  # Reduced to 2 epochs\n",
    "WARMUP_RATIO = 0.1\n",
    "N_FOLDS = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "OUTPUT_DIR = \"model_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = clean_create_vectors(df)\n",
    "y_columns = [c for c in df.columns if c not in [\"journal\", \"emotion_vectors\", \"activity_vectors\"]]\n",
    "\n",
    "# remove labels with <10 samples \n",
    "label_counts = df[y_columns].sum()\n",
    "unstable_labels = label_counts[label_counts < 10].index.tolist()\n",
    "if unstable_labels:\n",
    "    print(f\"Removing unstable labels with <10 samples: {unstable_labels}\")\n",
    "y_columns = [c for c in y_columns if c not in unstable_labels]\n",
    "label_names = y_columns\n",
    "num_labels = len(label_names)\n",
    "X = df[\"journal\"].tolist()\n",
    "y = df[label_names].astype(int).values\n",
    "\n",
    "# Fix Class Weight Calc\n",
    "class_counts = np.sum(y, axis=0)\n",
    "effective_num = 1.0 / (class_counts + 1e-9)\n",
    "weights = (1.0 / effective_num) / np.sum(1.0 / effective_num)\n",
    "class_weights = torch.tensor(weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "# Focal Loss def with class weights\n",
    "def focal_loss_fn(logits, targets, alpha=0.25, gamma=3.0, weights=None):\n",
    "    bce_loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "    if weights is not None:\n",
    "        bce_loss = bce_loss * weights\n",
    "    p_t = torch.exp(-bce_loss)\n",
    "    focal_loss = alpha * (1 - p_t) ** gamma * bce_loss\n",
    "    return focal_loss.mean()\n",
    "\n",
    "class JournalDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.enc = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            \"input_ids\": self.enc.input_ids[i],\n",
    "            \"attention_mask\": self.enc.attention_mask[i],\n",
    "            \"labels\": self.labels[i],\n",
    "        }\n",
    "\n",
    "# Threshold tuning with Asymmetric Probability Shift\n",
    "def find_optimal_thresholds(probs, true_labels, label_names):\n",
    "    shifted_probs = probs.copy()\n",
    "    for i in range(shifted_probs.shape[1]):\n",
    "        shifted_probs[:, i] = np.clip(shifted_probs[:, i] * 1.2 - 0.1, 0, 1)\n",
    "\n",
    "    thresholds = [v/100. for v in range(5, 100, 5)]\n",
    "    best_thresholds = {}\n",
    "    for i, label in enumerate(label_names):\n",
    "        best_score, best_thresh = 0, 0.5\n",
    "        for t in thresholds:\n",
    "            preds = (shifted_probs[:, i] >= t).astype(int)\n",
    "            f1 = f1_score(true_labels[:, i], preds, zero_division=0)\n",
    "            if f1 > best_score:\n",
    "                best_score, best_thresh = f1, t\n",
    "        best_thresholds[label] = best_thresh\n",
    "        print(f\"Label: {label} | Best F1={best_score:.3f} @ threshold={best_thresh}\")\n",
    "    return best_thresholds\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "    all_test_logits = np.zeros((len(X_test), num_labels))\n",
    "    all_val_probs = np.zeros((len(X_trainval), num_labels))\n",
    "    all_val_labels = np.zeros((len(X_trainval), num_labels))\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(mskf.split(X_trainval, y_trainval), 1):\n",
    "        print(f\"\\n=== Fold {fold}/{N_FOLDS} ===\")\n",
    "        X_tr = [X_trainval[i] for i in tr_idx]; y_tr = y_trainval[tr_idx]\n",
    "        X_val = [X_trainval[i] for i in val_idx]; y_val = y_trainval[val_idx]\n",
    "\n",
    "        tr_ds = JournalDataset(X_tr, y_tr, tokenizer)\n",
    "        val_ds = JournalDataset(X_val, y_val, tokenizer)\n",
    "        test_ds = JournalDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "        tr_dl = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "        test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=num_labels,\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        ).to(DEVICE)\n",
    "        model.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        optimizer = AdamW(model.parameters(), lr=LR)\n",
    "        total_steps = len(tr_dl) * NUM_EPOCHS\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(total_steps * WARMUP_RATIO),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "p\n",
    "        for epoch in range(1, NUM_EPOCHS+1):\n",
    "            model.train()\n",
    "            for batch in tqdm(tr_dl, desc=f\"Fold{fold} Train E{epoch}\"):\n",
    "                optimizer.zero_grad()\n",
    "                inputs = {k: v.to(DEVICE) for k, v in batch.items() if k != \"labels\"}\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "                logits = model(**inputs).logits\n",
    "                loss = focal_loss_fn(logits, labels, weights=class_weights)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_logits, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                inputs = {k: v.to(DEVICE) for k, v in batch.items() if k != \"labels\"}\n",
    "                logits = model(**inputs).logits.cpu().numpy()\n",
    "                val_logits.append(logits)\n",
    "                val_labels.append(batch[\"labels\"].numpy())\n",
    "        val_logits = np.vstack(val_logits)\n",
    "        val_probs = torch.sigmoid(torch.tensor(val_logits)).numpy()\n",
    "        all_val_probs[val_idx] = val_probs\n",
    "        all_val_labels[val_idx] = y_val\n",
    "\n",
    "        fold_test_logits = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dl:\n",
    "                inputs = {k: v.to(DEVICE) for k, v in batch.items() if k != \"labels\"}\n",
    "                logits = model(**inputs).logits.cpu().numpy()\n",
    "                fold_test_logits.append(logits)\n",
    "        all_test_logits += np.vstack(fold_test_logits)\n",
    "    avg_test_logits = all_test_logits / N_FOLDS\n",
    "    avg_test_probs = torch.sigmoid(torch.tensor(avg_test_logits)).numpy()\n",
    "\n",
    "    best_thresholds = find_optimal_thresholds(all_val_probs, all_val_labels, label_names)\n",
    "\n",
    "    final_preds = np.zeros_like(avg_test_probs, dtype=int)\n",
    "    for i, label in enumerate(label_names):\n",
    "        t = best_thresholds[label]\n",
    "        final_preds[:, i] = (avg_test_probs[:, i] >= t).astype(int)\n",
    "\n",
    "    print(\"\\nFinal Classification Report:\\n\")\n",
    "    print(classification_report(\n",
    "        y_test, final_preds,\n",
    "        target_names=label_names,\n",
    "        zero_division=0,\n",
    "        sample_weight=(y_test.sum(axis=1) > 0)\n",
    "    ))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
